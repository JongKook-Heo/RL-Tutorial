{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import collections, random\n",
    "import argparse\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='Soft Actor Critic')\n",
    "    parser.add_argument('--lr-pi', default=5e-4)\n",
    "    parser.add_argument('--lr-q', default=1e-3)\n",
    "    parser.add_argument('--lr-alpha', default=1e-3)\n",
    "    parser.add_argument('--alpha_init', default=1e-2)\n",
    "    parser.add_argument('--gamma', default=0.98)\n",
    "    parser.add_argument('--batch-size', default=32)\n",
    "    parser.add_argument('--buffer-size', default=5e4, type=int)\n",
    "    parser.add_argument('--tau', default=1e-2)\n",
    "    parser.add_argument('--target-entropy', default=-1.0)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size, device):\n",
    "        self.buffer = collections.deque(maxlen=int(buffer_size))\n",
    "        self.device = device\n",
    "    \n",
    "    def append(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = torch.FloatTensor([t[0] for t in batch]).to(self.device)\n",
    "        a_batch = torch.FloatTensor([[t[1]] for t in batch]).to(self.device)\n",
    "        r_batch = torch.FloatTensor([[t[2]] for t in batch]).to(self.device)\n",
    "        s2_batch = torch.FloatTensor([t[3] for t in batch]).to(self.device)\n",
    "        done_batch = torch.FloatTensor([[0.0] if t[4] else [1.0] for t in batch]).to(self.device)\n",
    "\n",
    "        return s_batch, a_batch, r_batch, s2_batch, done_batch\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self, n_states):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc_state = nn.Linear(n_states, 64)\n",
    "        self.fc_action = nn.Linear(1, 64)\n",
    "        self.fc_concat = nn.Linear(64 * 2 , 32)\n",
    "        self.fc_out = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        h1 = F.relu(self.fc_state(state))\n",
    "        h2 = F.relu(self.fc_action(action))\n",
    "        h_concat = F.relu(self.fc_concat(torch.cat([h1, h2], dim=1)))\n",
    "        q = self.fc_out(h_concat)\n",
    "        return q\n",
    "\n",
    "\n",
    "class TwinQnet(nn.Module):\n",
    "    def __init__(self, n_states):\n",
    "        super(TwinQnet, self).__init__()\n",
    "        self.Qnet1, self.Qnet2 = [Qnet(n_states)] * 2\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q1 = self.Qnet1(state, action)\n",
    "        q2 = self.Qnet2(state, action)\n",
    "        return q1, q2\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Critic, self).__init__()\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        self.current_twin = TwinQnet(self.n_states)\n",
    "        self.target_twin = TwinQnet(self.n_states)\n",
    "        self.hard_update()\n",
    "        self.q1_optim = optim.Adam(self.current_twin.Qnet1.parameters(), lr=self.lr_q)\n",
    "        self.q2_optim = optim.Adam(self.current_twin.Qnet2.parameters(), lr=self.lr_q)\n",
    "\n",
    "    def forward(self, state, action, target=False):\n",
    "        if target:\n",
    "            return self.target_twin(state, action)\n",
    "        return self.current_twin(state, action)\n",
    "\n",
    "    def train_net(self, target, batch):\n",
    "        s, a, _, _, _ = batch\n",
    "        q1, q2 = self.forward(s, a)\n",
    "        q1_loss = F.smooth_l1_loss(q1, target)\n",
    "        q2_loss = F.smooth_l1_loss(q2, target)\n",
    "        \n",
    "        self.q1_optim.zero_grad()\n",
    "        self.q2_optim.zero_grad()\n",
    "        \n",
    "        q1_loss.mean().backward()\n",
    "        q2_loss.mean().backward()\n",
    "\n",
    "        self.q1_optim.step()\n",
    "        self.q2_optim.step()\n",
    "        \n",
    "    def soft_update(self):\n",
    "        for t, s in zip(self.target_twin.parameters(), self.current_twin.parameters()):\n",
    "            t.data.copy_(t.data * (1.0 - self.tau) + s.data * self.tau)\n",
    "\n",
    "    def hard_update(self):\n",
    "        self.target_twin.load_state_dict(self.current_twin.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Actor, self).__init__()\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "        self.fc_state = nn.Linear(self.n_states, 128)\n",
    "        self.fc_mu = nn.Linear(128, 1)\n",
    "        self.fc_std = nn.Linear(128, 1)\n",
    "        self.fc_out = nn.Linear(32, 1)\n",
    "        self.actor_optim = optim.Adam(self.parameters(), lr=self.lr_pi)\n",
    "\n",
    "        self.log_alpha = torch.tensor(np.log(self.alpha_init))\n",
    "        self.log_alpha.requires_grad = True\n",
    "        self.log_alpha_optim = optim.Adam([self.log_alpha], lr=self.lr_alpha)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc_state(x))\n",
    "        mu, std = self.fc_mu(x), F.softplus(self.fc_std(x))\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.rsample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        bounded_action = torch.tanh(action)\n",
    "        bounded_log_prob = log_prob - torch.log(1-torch.tanh(action).pow(2) + 1e-7)\n",
    "        return bounded_action, bounded_log_prob\n",
    "    \n",
    "    def train_net(self, critic, batch):\n",
    "        s, _, _, _ , _ = batch\n",
    "        a, log_prob = self.forward(s)\n",
    "\n",
    "        q1_val, q2_val = critic(s, a)\n",
    "        min_q = torch.min(q1_val, q2_val)\n",
    "\n",
    "        actor_loss = self.log_alpha.exp() * log_prob - min_q\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.mean().backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        self.log_alpha_optim.zero_grad()\n",
    "        alpha_loss = - (self.log_alpha.exp() * (log_prob + self.target_entropy).detach()).mean()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optim.step()\n",
    "\n",
    "\n",
    "class SAC():\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "        actor_configs = {k: kwargs[k] for k in ['n_states', 'lr_pi', 'lr_alpha', 'alpha_init', 'target_entropy']}\n",
    "        critic_configs = {k: kwargs[k] for k in ['n_states', 'lr_q', 'tau']}\n",
    "        buffer_configs = {k: kwargs[k] for k in ['buffer_size', 'device']}\n",
    "        self.actor = Actor(**actor_configs).to(self.device)\n",
    "        self.critic = Critic(**critic_configs).to(self.device)\n",
    "        self.buffer = ReplayBuffer(**buffer_configs)\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def calc_q_target(self, batch):\n",
    "        s, a, r, s2, done = batch\n",
    "\n",
    "        a2, log_prob = self.actor(s2)\n",
    "        q1_val, q2_val = self.critic(s2, a2, target=True)\n",
    "        min_q = torch.min(q1_val, q2_val)\n",
    "        entropy = -self.actor.log_alpha.exp() * log_prob\n",
    "        \n",
    "        target = r + self.gamma * done * (min_q + entropy)\n",
    "\n",
    "        return target\n",
    "\n",
    "    def learn(self):\n",
    "        batch = self.buffer.sample(self.batch_size)\n",
    "        td_target = self.calc_q_target(batch)\n",
    "\n",
    "        self.critic.train_net(td_target, batch)\n",
    "        self.actor.train_net(self.critic, batch)\n",
    "        self.critic.soft_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of ep : 0020 | Avg. score : -1535.2 | alpha : 0.0078\n",
      "# of ep : 0040 | Avg. score : -1671.2 | alpha : 0.0059\n",
      "# of ep : 0060 | Avg. score : -1496.6 | alpha : 0.0063\n",
      "# of ep : 0080 | Avg. score : -1317.6 | alpha : 0.0076\n",
      "# of ep : 0100 | Avg. score : -1332.9 | alpha : 0.0103\n",
      "# of ep : 0120 | Avg. score : -1304.5 | alpha : 0.0141\n",
      "# of ep : 0140 | Avg. score : -1227.1 | alpha : 0.0189\n",
      "# of ep : 0160 | Avg. score : -1205.4 | alpha : 0.0182\n",
      "# of ep : 0180 | Avg. score : -1073.3 | alpha : 0.0162\n",
      "# of ep : 0200 | Avg. score : -1088.6 | alpha : 0.0161\n",
      "# of ep : 0220 | Avg. score : -1083.8 | alpha : 0.0139\n",
      "# of ep : 0240 | Avg. score : -907.7 | alpha : 0.0158\n",
      "# of ep : 0260 | Avg. score : -928.6 | alpha : 0.0151\n",
      "# of ep : 0280 | Avg. score : -945.4 | alpha : 0.0187\n",
      "# of ep : 0300 | Avg. score : -799.5 | alpha : 0.0283\n",
      "# of ep : 0320 | Avg. score : -752.0 | alpha : 0.0289\n",
      "# of ep : 0340 | Avg. score : -822.8 | alpha : 0.0295\n",
      "# of ep : 0360 | Avg. score : -965.3 | alpha : 0.0276\n",
      "# of ep : 0380 | Avg. score : -982.4 | alpha : 0.0267\n",
      "# of ep : 0400 | Avg. score : -858.0 | alpha : 0.0222\n",
      "# of ep : 0420 | Avg. score : -849.9 | alpha : 0.0182\n",
      "# of ep : 0440 | Avg. score : -712.2 | alpha : 0.0148\n",
      "# of ep : 0460 | Avg. score : -690.0 | alpha : 0.0127\n",
      "# of ep : 0480 | Avg. score : -724.1 | alpha : 0.0112\n",
      "# of ep : 0500 | Avg. score : -689.9 | alpha : 0.0093\n",
      "# of ep : 0520 | Avg. score : -690.1 | alpha : 0.0085\n",
      "# of ep : 0540 | Avg. score : -667.2 | alpha : 0.0087\n",
      "# of ep : 0560 | Avg. score : -813.2 | alpha : 0.0116\n",
      "# of ep : 0580 | Avg. score : -613.4 | alpha : 0.0140\n",
      "# of ep : 0600 | Avg. score : -563.4 | alpha : 0.0131\n",
      "# of ep : 0620 | Avg. score : -469.2 | alpha : 0.0109\n",
      "# of ep : 0640 | Avg. score : -538.5 | alpha : 0.0100\n",
      "# of ep : 0660 | Avg. score : -531.4 | alpha : 0.0101\n",
      "# of ep : 0680 | Avg. score : -419.6 | alpha : 0.0113\n",
      "# of ep : 0700 | Avg. score : -399.1 | alpha : 0.0128\n",
      "# of ep : 0720 | Avg. score : -270.7 | alpha : 0.0165\n",
      "# of ep : 0740 | Avg. score : -395.7 | alpha : 0.0222\n",
      "# of ep : 0760 | Avg. score : -403.4 | alpha : 0.0217\n",
      "# of ep : 0780 | Avg. score : -377.9 | alpha : 0.0212\n",
      "# of ep : 0800 | Avg. score : -306.9 | alpha : 0.0230\n",
      "# of ep : 0820 | Avg. score : -317.8 | alpha : 0.0219\n",
      "# of ep : 0840 | Avg. score : -270.5 | alpha : 0.0186\n",
      "# of ep : 0860 | Avg. score : -223.9 | alpha : 0.0171\n",
      "# of ep : 0880 | Avg. score : -207.0 | alpha : 0.0163\n",
      "# of ep : 0900 | Avg. score : -214.0 | alpha : 0.0165\n",
      "# of ep : 0920 | Avg. score : -167.6 | alpha : 0.0163\n",
      "# of ep : 0940 | Avg. score : -217.3 | alpha : 0.0169\n",
      "# of ep : 0960 | Avg. score : -237.6 | alpha : 0.0170\n",
      "# of ep : 0980 | Avg. score : -169.9 | alpha : 0.0174\n",
      "# of ep : 1000 | Avg. score : -276.5 | alpha : 0.0187\n",
      "# of ep : 1020 | Avg. score : -208.7 | alpha : 0.0164\n",
      "# of ep : 1040 | Avg. score : -175.9 | alpha : 0.0144\n",
      "# of ep : 1060 | Avg. score : -154.9 | alpha : 0.0115\n",
      "# of ep : 1080 | Avg. score : -182.5 | alpha : 0.0088\n",
      "# of ep : 1100 | Avg. score : -177.2 | alpha : 0.0062\n",
      "# of ep : 1120 | Avg. score : -172.6 | alpha : 0.0043\n",
      "# of ep : 1140 | Avg. score : -159.1 | alpha : 0.0031\n",
      "# of ep : 1160 | Avg. score : -280.2 | alpha : 0.0024\n",
      "# of ep : 1180 | Avg. score : -164.2 | alpha : 0.0021\n",
      "# of ep : 1200 | Avg. score : -197.7 | alpha : 0.0017\n",
      "# of ep : 1220 | Avg. score : -232.9 | alpha : 0.0014\n",
      "# of ep : 1240 | Avg. score : -190.4 | alpha : 0.0012\n",
      "# of ep : 1260 | Avg. score : -293.1 | alpha : 0.0010\n",
      "# of ep : 1280 | Avg. score : -242.1 | alpha : 0.0010\n",
      "# of ep : 1300 | Avg. score : -233.3 | alpha : 0.0012\n",
      "# of ep : 1320 | Avg. score : -161.8 | alpha : 0.0012\n",
      "# of ep : 1340 | Avg. score : -146.2 | alpha : 0.0010\n",
      "# of ep : 1360 | Avg. score : -169.7 | alpha : 0.0008\n",
      "# of ep : 1380 | Avg. score : -160.8 | alpha : 0.0007\n",
      "# of ep : 1400 | Avg. score : -185.6 | alpha : 0.0006\n",
      "# of ep : 1420 | Avg. score : -131.6 | alpha : 0.0005\n",
      "# of ep : 1440 | Avg. score : -164.7 | alpha : 0.0005\n",
      "# of ep : 1460 | Avg. score : -203.8 | alpha : 0.0006\n",
      "# of ep : 1480 | Avg. score : -150.9 | alpha : 0.0005\n",
      "# of ep : 1500 | Avg. score : -124.5 | alpha : 0.0004\n",
      "# of ep : 1520 | Avg. score : -162.0 | alpha : 0.0004\n",
      "# of ep : 1540 | Avg. score : -147.7 | alpha : 0.0003\n",
      "# of ep : 1560 | Avg. score : -220.2 | alpha : 0.0002\n",
      "# of ep : 1580 | Avg. score : -185.6 | alpha : 0.0002\n",
      "# of ep : 1600 | Avg. score : -218.4 | alpha : 0.0002\n",
      "# of ep : 1620 | Avg. score : -193.0 | alpha : 0.0002\n",
      "# of ep : 1640 | Avg. score : -178.5 | alpha : 0.0003\n",
      "# of ep : 1660 | Avg. score : -197.4 | alpha : 0.0004\n",
      "# of ep : 1680 | Avg. score : -287.8 | alpha : 0.0005\n",
      "# of ep : 1700 | Avg. score : -990.5 | alpha : 0.0006\n",
      "# of ep : 1720 | Avg. score : -766.4 | alpha : 0.0010\n",
      "# of ep : 1740 | Avg. score : -465.1 | alpha : 0.0017\n",
      "# of ep : 1760 | Avg. score : -412.7 | alpha : 0.0039\n",
      "# of ep : 1780 | Avg. score : -238.7 | alpha : 0.0072\n",
      "# of ep : 1800 | Avg. score : -168.4 | alpha : 0.0128\n",
      "# of ep : 1820 | Avg. score : -182.0 | alpha : 0.0162\n",
      "# of ep : 1840 | Avg. score : -211.8 | alpha : 0.0178\n",
      "# of ep : 1860 | Avg. score : -180.2 | alpha : 0.0208\n",
      "# of ep : 1880 | Avg. score : -214.8 | alpha : 0.0250\n",
      "# of ep : 1900 | Avg. score : -158.0 | alpha : 0.0259\n",
      "# of ep : 1920 | Avg. score : -166.7 | alpha : 0.0283\n",
      "# of ep : 1940 | Avg. score : -207.0 | alpha : 0.0280\n",
      "# of ep : 1960 | Avg. score : -158.5 | alpha : 0.0281\n",
      "# of ep : 1980 | Avg. score : -214.5 | alpha : 0.0272\n",
      "# of ep : 2000 | Avg. score : -148.4 | alpha : 0.0247\n",
      "# of ep : 2020 | Avg. score : -143.9 | alpha : 0.0219\n",
      "# of ep : 2040 | Avg. score : -161.6 | alpha : 0.0199\n",
      "# of ep : 2060 | Avg. score : -154.1 | alpha : 0.0198\n",
      "# of ep : 2080 | Avg. score : -164.4 | alpha : 0.0185\n",
      "# of ep : 2100 | Avg. score : -185.0 | alpha : 0.0161\n",
      "# of ep : 2120 | Avg. score : -173.7 | alpha : 0.0138\n",
      "# of ep : 2140 | Avg. score : -127.6 | alpha : 0.0113\n",
      "# of ep : 2160 | Avg. score : -140.2 | alpha : 0.0084\n",
      "# of ep : 2180 | Avg. score : -147.3 | alpha : 0.0060\n",
      "# of ep : 2200 | Avg. score : -139.6 | alpha : 0.0044\n",
      "# of ep : 2220 | Avg. score : -217.9 | alpha : 0.0033\n",
      "# of ep : 2240 | Avg. score : -151.5 | alpha : 0.0026\n",
      "# of ep : 2260 | Avg. score : -185.7 | alpha : 0.0021\n",
      "# of ep : 2280 | Avg. score : -191.4 | alpha : 0.0018\n",
      "# of ep : 2300 | Avg. score : -175.0 | alpha : 0.0015\n",
      "# of ep : 2320 | Avg. score : -139.7 | alpha : 0.0012\n",
      "# of ep : 2340 | Avg. score : -155.5 | alpha : 0.0011\n",
      "# of ep : 2360 | Avg. score : -164.0 | alpha : 0.0009\n",
      "# of ep : 2380 | Avg. score : -170.9 | alpha : 0.0008\n",
      "# of ep : 2400 | Avg. score : -233.3 | alpha : 0.0007\n",
      "# of ep : 2420 | Avg. score : -205.7 | alpha : 0.0007\n",
      "# of ep : 2440 | Avg. score : -245.5 | alpha : 0.0006\n",
      "# of ep : 2460 | Avg. score : -179.6 | alpha : 0.0006\n",
      "# of ep : 2480 | Avg. score : -180.9 | alpha : 0.0005\n",
      "# of ep : 2500 | Avg. score : -180.0 | alpha : 0.0007\n",
      "# of ep : 2520 | Avg. score : -133.9 | alpha : 0.0010\n",
      "# of ep : 2540 | Avg. score : -144.2 | alpha : 0.0009\n",
      "# of ep : 2560 | Avg. score : -177.1 | alpha : 0.0008\n",
      "# of ep : 2580 | Avg. score : -197.5 | alpha : 0.0007\n",
      "# of ep : 2600 | Avg. score : -153.6 | alpha : 0.0007\n",
      "# of ep : 2620 | Avg. score : -164.9 | alpha : 0.0007\n",
      "# of ep : 2640 | Avg. score : -218.1 | alpha : 0.0008\n",
      "# of ep : 2660 | Avg. score : -162.7 | alpha : 0.0009\n",
      "# of ep : 2680 | Avg. score : -195.1 | alpha : 0.0009\n",
      "# of ep : 2700 | Avg. score : -252.1 | alpha : 0.0009\n",
      "# of ep : 2720 | Avg. score : -210.2 | alpha : 0.0012\n",
      "# of ep : 2740 | Avg. score : -241.5 | alpha : 0.0015\n",
      "# of ep : 2760 | Avg. score : -183.2 | alpha : 0.0013\n",
      "# of ep : 2780 | Avg. score : -211.1 | alpha : 0.0011\n",
      "# of ep : 2800 | Avg. score : -177.1 | alpha : 0.0010\n",
      "# of ep : 2820 | Avg. score : -146.7 | alpha : 0.0009\n",
      "# of ep : 2840 | Avg. score : -136.1 | alpha : 0.0008\n",
      "# of ep : 2860 | Avg. score : -169.2 | alpha : 0.0007\n",
      "# of ep : 2880 | Avg. score : -147.9 | alpha : 0.0007\n",
      "# of ep : 2900 | Avg. score : -209.5 | alpha : 0.0006\n",
      "# of ep : 2920 | Avg. score : -183.6 | alpha : 0.0004\n",
      "# of ep : 2940 | Avg. score : -161.8 | alpha : 0.0004\n",
      "# of ep : 2960 | Avg. score : -130.3 | alpha : 0.0003\n",
      "# of ep : 2980 | Avg. score : -256.7 | alpha : 0.0003\n",
      "# of ep : 3000 | Avg. score : -260.1 | alpha : 0.0003\n",
      "# of ep : 3020 | Avg. score : -170.2 | alpha : 0.0003\n",
      "# of ep : 3040 | Avg. score : -138.6 | alpha : 0.0002\n",
      "# of ep : 3060 | Avg. score : -171.4 | alpha : 0.0002\n",
      "# of ep : 3080 | Avg. score : -184.5 | alpha : 0.0002\n",
      "# of ep : 3100 | Avg. score : -178.4 | alpha : 0.0002\n",
      "# of ep : 3120 | Avg. score : -134.5 | alpha : 0.0003\n",
      "# of ep : 3140 | Avg. score : -137.1 | alpha : 0.0004\n",
      "# of ep : 3160 | Avg. score : -156.3 | alpha : 0.0004\n",
      "# of ep : 3180 | Avg. score : -205.3 | alpha : 0.0004\n",
      "# of ep : 3200 | Avg. score : -128.2 | alpha : 0.0005\n",
      "# of ep : 3220 | Avg. score : -137.6 | alpha : 0.0006\n",
      "# of ep : 3240 | Avg. score : -150.7 | alpha : 0.0007\n",
      "# of ep : 3260 | Avg. score : -194.4 | alpha : 0.0007\n",
      "# of ep : 3280 | Avg. score : -168.4 | alpha : 0.0007\n",
      "# of ep : 3300 | Avg. score : -193.1 | alpha : 0.0006\n",
      "# of ep : 3320 | Avg. score : -232.9 | alpha : 0.0007\n",
      "# of ep : 3340 | Avg. score : -207.5 | alpha : 0.0007\n",
      "# of ep : 3360 | Avg. score : -182.4 | alpha : 0.0006\n",
      "# of ep : 3380 | Avg. score : -188.9 | alpha : 0.0005\n",
      "# of ep : 3400 | Avg. score : -180.0 | alpha : 0.0004\n",
      "# of ep : 3420 | Avg. score : -200.6 | alpha : 0.0004\n",
      "# of ep : 3440 | Avg. score : -187.3 | alpha : 0.0003\n",
      "# of ep : 3460 | Avg. score : -222.4 | alpha : 0.0003\n",
      "# of ep : 3480 | Avg. score : -197.7 | alpha : 0.0003\n",
      "# of ep : 3500 | Avg. score : -164.6 | alpha : 0.0003\n",
      "# of ep : 3520 | Avg. score : -190.0 | alpha : 0.0003\n",
      "# of ep : 3540 | Avg. score : -241.8 | alpha : 0.0004\n",
      "# of ep : 3560 | Avg. score : -206.3 | alpha : 0.0004\n",
      "# of ep : 3580 | Avg. score : -154.2 | alpha : 0.0004\n",
      "# of ep : 3600 | Avg. score : -151.7 | alpha : 0.0006\n",
      "# of ep : 3620 | Avg. score : -188.9 | alpha : 0.0007\n",
      "# of ep : 3640 | Avg. score : -199.0 | alpha : 0.0007\n",
      "# of ep : 3660 | Avg. score : -194.9 | alpha : 0.0009\n",
      "# of ep : 3680 | Avg. score : -185.2 | alpha : 0.0011\n",
      "# of ep : 3700 | Avg. score : -219.5 | alpha : 0.0011\n",
      "# of ep : 3720 | Avg. score : -205.6 | alpha : 0.0012\n",
      "# of ep : 3740 | Avg. score : -246.2 | alpha : 0.0012\n",
      "# of ep : 3760 | Avg. score : -177.5 | alpha : 0.0013\n",
      "# of ep : 3780 | Avg. score : -166.2 | alpha : 0.0013\n",
      "# of ep : 3800 | Avg. score : -182.9 | alpha : 0.0011\n",
      "# of ep : 3820 | Avg. score : -216.8 | alpha : 0.0009\n",
      "# of ep : 3840 | Avg. score : -156.4 | alpha : 0.0010\n",
      "# of ep : 3860 | Avg. score : -196.3 | alpha : 0.0013\n",
      "# of ep : 3880 | Avg. score : -220.2 | alpha : 0.0016\n",
      "# of ep : 3900 | Avg. score : -220.8 | alpha : 0.0019\n",
      "# of ep : 3920 | Avg. score : -203.9 | alpha : 0.0024\n",
      "# of ep : 3940 | Avg. score : -177.1 | alpha : 0.0030\n",
      "# of ep : 3960 | Avg. score : -220.2 | alpha : 0.0032\n",
      "# of ep : 3980 | Avg. score : -186.7 | alpha : 0.0035\n",
      "# of ep : 4000 | Avg. score : -188.9 | alpha : 0.0040\n",
      "# of ep : 4020 | Avg. score : -196.9 | alpha : 0.0050\n",
      "# of ep : 4040 | Avg. score : -242.6 | alpha : 0.0070\n",
      "# of ep : 4060 | Avg. score : -205.0 | alpha : 0.0095\n",
      "# of ep : 4080 | Avg. score : -187.0 | alpha : 0.0115\n",
      "# of ep : 4100 | Avg. score : -230.8 | alpha : 0.0149\n",
      "# of ep : 4120 | Avg. score : -233.9 | alpha : 0.0189\n",
      "# of ep : 4140 | Avg. score : -246.0 | alpha : 0.0231\n",
      "# of ep : 4160 | Avg. score : -219.3 | alpha : 0.0316\n",
      "# of ep : 4180 | Avg. score : -190.6 | alpha : 0.0390\n",
      "# of ep : 4200 | Avg. score : -259.0 | alpha : 0.0529\n",
      "# of ep : 4220 | Avg. score : -242.2 | alpha : 0.0702\n",
      "# of ep : 4240 | Avg. score : -264.3 | alpha : 0.0962\n",
      "# of ep : 4260 | Avg. score : -191.1 | alpha : 0.1365\n",
      "# of ep : 4280 | Avg. score : -198.1 | alpha : 0.1990\n",
      "# of ep : 4300 | Avg. score : -221.3 | alpha : 0.2635\n",
      "# of ep : 4320 | Avg. score : -314.2 | alpha : 0.3383\n",
      "# of ep : 4340 | Avg. score : -229.5 | alpha : 0.3958\n",
      "# of ep : 4360 | Avg. score : -242.1 | alpha : 0.4650\n",
      "# of ep : 4380 | Avg. score : -239.8 | alpha : 0.5211\n",
      "# of ep : 4400 | Avg. score : -282.7 | alpha : 0.5610\n",
      "# of ep : 4420 | Avg. score : -291.8 | alpha : 0.5662\n",
      "# of ep : 4440 | Avg. score : -244.4 | alpha : 0.4990\n",
      "# of ep : 4460 | Avg. score : -274.3 | alpha : 0.4243\n",
      "# of ep : 4480 | Avg. score : -286.8 | alpha : 0.3369\n",
      "# of ep : 4500 | Avg. score : -352.5 | alpha : 0.2460\n",
      "# of ep : 4520 | Avg. score : -314.8 | alpha : 0.1815\n",
      "# of ep : 4540 | Avg. score : -222.7 | alpha : 0.1389\n",
      "# of ep : 4560 | Avg. score : -215.7 | alpha : 0.1104\n",
      "# of ep : 4580 | Avg. score : -169.4 | alpha : 0.0864\n",
      "# of ep : 4600 | Avg. score : -170.8 | alpha : 0.0671\n",
      "# of ep : 4620 | Avg. score : -169.6 | alpha : 0.0523\n",
      "# of ep : 4640 | Avg. score : -166.0 | alpha : 0.0423\n",
      "# of ep : 4660 | Avg. score : -193.7 | alpha : 0.0349\n",
      "# of ep : 4680 | Avg. score : -186.2 | alpha : 0.0294\n",
      "# of ep : 4700 | Avg. score : -205.7 | alpha : 0.0251\n",
      "# of ep : 4720 | Avg. score : -183.8 | alpha : 0.0217\n",
      "# of ep : 4740 | Avg. score : -182.4 | alpha : 0.0186\n",
      "# of ep : 4760 | Avg. score : -175.5 | alpha : 0.0162\n",
      "# of ep : 4780 | Avg. score : -169.8 | alpha : 0.0140\n",
      "# of ep : 4800 | Avg. score : -189.5 | alpha : 0.0119\n",
      "# of ep : 4820 | Avg. score : -203.7 | alpha : 0.0102\n",
      "# of ep : 4840 | Avg. score : -215.8 | alpha : 0.0087\n",
      "# of ep : 4860 | Avg. score : -219.6 | alpha : 0.0075\n",
      "# of ep : 4880 | Avg. score : -205.1 | alpha : 0.0064\n",
      "# of ep : 4900 | Avg. score : -246.2 | alpha : 0.0058\n",
      "# of ep : 4920 | Avg. score : -160.8 | alpha : 0.0054\n",
      "# of ep : 4940 | Avg. score : -234.3 | alpha : 0.0049\n",
      "# of ep : 4960 | Avg. score : -277.2 | alpha : 0.0044\n",
      "# of ep : 4980 | Avg. score : -162.9 | alpha : 0.0041\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args([])\n",
    "    args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    args.n_states = env.observation_space.shape[0]\n",
    "\n",
    "    configs = dict(vars(args))\n",
    "    agent = SAC(**configs)\n",
    "\n",
    "    score = 0.0\n",
    "    interval = 20\n",
    "    action_limit = env.action_space.high[0]\n",
    "\n",
    "    for ep in range(10000):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            a, log_prob = agent.actor(torch.tensor(s, dtype=torch.float, device=agent.device))\n",
    "            s2, r, done, info = env.step([action_limit * a.item()])\n",
    "            agent.buffer.append((s, a.item(), r/10.0, s2, done))\n",
    "            score += r\n",
    "            s = s2\n",
    "        \n",
    "        if agent.buffer.size > 1e3:\n",
    "            for i in range(20):\n",
    "                agent.learn()\n",
    "        \n",
    "        if ep % interval ==0 and ep != 0:\n",
    "            avg_score = score/interval\n",
    "            alpha = agent.actor.log_alpha.exp()\n",
    "            print(f'# of ep : {ep:04d} | Avg. score : {avg_score:.1f} | alpha : {alpha:.4f}')\n",
    "            score = 0.0\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db2248d8238dd1c2a6a03e6bdeeec789cb65d3cb28d7ca5db10a000b8f87d294"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('atari': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
